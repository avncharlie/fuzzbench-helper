#!/usr/bin/env python3
"""get_fuzzer_speeds.py

Generate throughput statistics, CSV summaries, pretty‑printed text, and a
bar‑chart (PNG) for FuzzBench experiments **or** from a previously generated
CSV file.

Usage examples
--------------
Parse a FuzzBench experiment directory (creates CSV and PNG)::

    ./throughput_summary.py --exp-dir ~/fb-data/my_exp

Render a chart from an existing CSV (no need for the experiments folder)::

    ./throughput_summary.py --csv ~/fb-data/my_exp/throughput.csv
"""

from __future__ import annotations

import sys
import tarfile
import argparse
import statistics
import csv
from collections import defaultdict
from pathlib import Path
from typing import Dict, Tuple, List
from typing import Dict, Tuple, List, Optional

# ── plotting ──────────────────────────────────────────────────────────────
import matplotlib

matplotlib.use("Agg")  # so the script works on head‑less servers
import matplotlib.pyplot as plt
import numpy as np
# ───────────────────────────────────────────────────────────────────────────

# ---- colour helpers -------------------------------------------------------
RESET = "\033[0m"
BOLD = "\033[1m"
CYAN = "\033[36m"  # headers
MAGENTA = "\033[35m"  # fuzzer names
GREEN = "\033[32m"  # fastest fuzzer for a benchmark
YELLOW = "\033[33m"  # everyone else
# --------------------------------------------------------------------------

DBG = False

# Result‑dict type alias
TrialMap = Dict[int, Tuple[float, Optional[float]]]
Results = Dict[str, Dict[str, Tuple[TrialMap, float, float]]]

# ───────────────────────────────────────────────────────────────────────────
# argument parsing
# ───────────────────────────────────────────────────────────────────────────

def parse_args() -> argparse.Namespace:
    """Return parsed command‑line arguments."""

    global DBG

    p = argparse.ArgumentParser(
        description=(
            "Summarise a FuzzBench experiment OR render statistics from an "
            "existing CSV; always emits a PNG bar‑chart."
        )
    )

    src = p.add_mutually_exclusive_group(required=True)
    src.add_argument(
        "--exp-dir",
        metavar="DIR",
        help="FuzzBench experiment directory (must contain a 'data' subfolder)",
    )
    src.add_argument(
        "--csv",
        dest="csv_path",
        metavar="FILE",
        help="Path to throughput CSV generated by a previous run",
    )
    p.add_argument(
        "--show-trial-ids",
        action="store_true",
        help="Include trial IDs next to speeds in the pretty-printed summary",
    )


    p.add_argument("--debug", action="store_true", help="Verbose prints for troubleshooting")
    return p.parse_args()


# ───────────────────────────────────────────────────────────────────────────
# helpers for parsing experiment data
# ───────────────────────────────────────────────────────────────────────────

def _parse_trial_id(trial_dir: Path) -> int:
    """Extract the integer trial id from folder name like 'trial-1091'."""
    name = trial_dir.name
    if name.startswith("trial-"):
        return int(name.split("-", 1)[1])
    raise ValueError(f"Unexpected trial dir name: {name!r}")

def _metrics_from_fuzzer_stats(stats: str) -> Tuple[float, Optional[float]]:
    """
    Return (execs_per_sec, run_time_seconds_or_None).

    AFL++ fuzzer_stats typically includes 'execs_per_sec' and 'run_time'.
    'run_time' may be seconds or milliseconds depending on build; we try to
    infer units conservatively.
    """
    kv = {}
    for line in stats.splitlines():
        if ":" in line:
            k, v = line.split(":", 1)
            kv[k.strip()] = v.strip()

    if "execs_per_sec" not in kv:
        raise ValueError("'execs_per_sec' not found in fuzzer_stats")

    execs_per_sec = float(kv["execs_per_sec"])
    run_time_s: Optional[float] = None

    def _as_float(x: str) -> Optional[float]:
        try:
            return float(x)
        except Exception:
            return None

    # Prefer explicit run_time when present
    if "run_time" in kv:
        rt = _as_float(kv["run_time"])
        if rt is not None:
            # Heuristic: if it's very large, it's likely milliseconds
            # (e.g., > 1e5 means > ~1.1 days if seconds, which is uncommon).
            # Treat values >= 1e6 as ms; 1e5-1e6 as ms too (safer).
            if rt >= 1e5:
                run_time_s = rt / 1000.0
            else:
                run_time_s = rt

    # Fall back to last_update - start_time if available (epoch ms)
    if run_time_s is None and "last_update" in kv and "start_time" in kv:
        lu = _as_float(kv["last_update"])
        st = _as_float(kv["start_time"])
        if lu is not None and st is not None and lu >= st:
            # Many builds store these as milliseconds
            diff = lu - st
            run_time_s = diff / 1000.0 if diff > 1e4 else diff

    return execs_per_sec, run_time_s

def _trial_metrics(trial: Path) -> Optional[Tuple[float, Optional[float]]]:
    """
    Scan the latest corpus archive in a trial dir and return (execs_per_sec, run_time_s).
    Returns None if no usable stats are found.
    """
    corpus_dir = trial / "corpus"

    def archive_no(p: Path) -> int:
        return int(p.name.replace(".tar.gz", "").replace("corpus-archive-", ""))

    try:
        tar_archives = [
            p for p in corpus_dir.iterdir()
            if p.name.startswith("corpus-archive-") and p.name.endswith(".tar.gz")
        ]
    except FileNotFoundError:
        return None

    for t in sorted(tar_archives, key=archive_no, reverse=True):
        try:
            with tarfile.open(t, "r:gz", errorlevel=0) as tf:
                for m in tf.getmembers():
                    if m.name.endswith("fuzzer_stats"):
                        if DBG:
                            print(f"Found fuzzer_stats in {t}")
                        stats = tf.extractfile(m).read().decode()
                        return _metrics_from_fuzzer_stats(stats)
        except tarfile.ReadError as e:
            print(f"Issue with tar file {t} ...")
            print(e)
            continue
    return None

def _experiment_results(base_dir: Path) -> Results:
    """Return results dict parsed from *base_dir* (.experiment-folders)."""

    results: Results = defaultdict(dict)

    for folder in filter(Path.is_dir, base_dir.iterdir()):
        fuzzer = folder.name.split("-")[-1]
        benchmark = folder.name[: -(len(fuzzer) + 1)]

        trial_map: TrialMap = {}
        for p in folder.iterdir():
            if not p.is_dir():
                continue
            try:
                tid = _parse_trial_id(p)
            except Exception:
                continue
            met = _trial_metrics(p)
            if met is None:
                continue
            execs_per_sec, run_time_s = met
            trial_map[tid] = (execs_per_sec, run_time_s)

        if not trial_map:
            print(f"{folder.name} has no data yet — skipping…")
            continue

        speeds = [v[0] for v in trial_map.values()]
        mean = statistics.mean(speeds)
        stdev = statistics.stdev(speeds) if len(speeds) > 1 else 0.0
        results[benchmark][fuzzer] = (trial_map, mean, stdev)

    return results


# ───────────────────────────────────────────────────────────────────────────
# CSV I/O
# ───────────────────────────────────────────────────────────────────────────

def write_csv(results: Results, out_path: Path) -> None:
    """Write *results* as CSV to *out_path*.
    - 'trials' now stores 'trial_id:execs_per_sec' pairs
    - 'trial_times' (optional) stores 'trial_id:seconds' pairs when available
    """

    with out_path.open("w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["benchmark", "fuzzer", "mean", "stdev", "num_trials", "trials", "trial_times"])
        for benchmark in sorted(results):
            for fuzzer in sorted(results[benchmark]):
                trial_map, mean, stdev = results[benchmark][fuzzer]
                pairs = " ".join(
                    f"{tid}:{speed:.2f}"
                    for tid, (speed, _rt) in sorted(trial_map.items())
                )
                time_pairs = " ".join(
                    f"{tid}:{int(rt)}"  # seconds, integer for compactness
                    for tid, (_speed, rt) in sorted(trial_map.items())
                    if rt is not None
                )
                w.writerow(
                    [
                        benchmark,
                        fuzzer,
                        f"{mean:.2f}",
                        f"{stdev:.2f}",
                        len(trial_map),
                        pairs,
                        time_pairs,
                    ]
                )

def read_csv(csv_path: Path) -> Results:
    """Return a results dict parsed from *csv_path*.

    Backward compatible:
    - If 'trials' is a space-separated list of floats (old format), we synthesize
      trial ids (1..N).
    - If 'trials' is 'tid:val' pairs (new format), we keep the mapping.
    - If 'trial_times' present, we attach times when possible.
    """
    results: Results = defaultdict(dict)

    with csv_path.open(newline="") as f:
        rdr = csv.DictReader(f)
        fields = rdr.fieldnames or []
        has_times = "trial_times" in fields

        for row in rdr:
            benchmark = row["benchmark"]
            fuzzer = row["fuzzer"]
            mean = float(row["mean"])
            stdev = float(row["stdev"])

            trial_map: TrialMap = {}
            trials_str = row.get("trials", "").strip()

            if ":" in trials_str:
                # new format: "1091:204.5 1092:207.2 ..."
                for item in trials_str.split():
                    tid_s, val_s = item.split(":", 1)
                    trial_map[int(tid_s)] = (float(val_s), None)
            elif trials_str:
                # old format: "204.5 207.2 ..." – synthesize ids 1..N
                vals = [float(x) for x in trials_str.split()]
                for i, v in enumerate(vals, 1):
                    trial_map[i] = (v, None)

            if has_times:
                times_str = row.get("trial_times", "").strip()
                if times_str:
                    for item in times_str.split():
                        tid_s, t_s = item.split(":", 1)
                        tid = int(tid_s)
                        tval = float(t_s)
                        if tid in trial_map:
                            speed, _ = trial_map[tid]
                            trial_map[tid] = (speed, tval)
                        else:
                            trial_map[tid] = (float("nan"), tval)

            results[benchmark][fuzzer] = (trial_map, mean, stdev)

    return results


# ───────────────────────────────────────────────────────────────────────────
# presentation helpers
# ───────────────────────────────────────────────────────────────────────────

# change signature
def pretty_print(results: Results, show_trial_ids: bool = False) -> None:
    """Emit colourful text summary to stdout."""

    for benchmark, fuzzers_dict in results.items():
        print(f"{BOLD}{CYAN}{benchmark}{RESET}")
        # sort fuzzers by mean exec/sec (desc)
        fuzzers_sorted = sorted(fuzzers_dict, key=lambda f: fuzzers_dict[f][1], reverse=True)
        fastest = fuzzers_sorted[0]

        for fuzzer in fuzzers_sorted:
            trial_map, mean, stdev = fuzzers_dict[fuzzer]
            colour = GREEN if fuzzer == fastest else YELLOW
            stdev_part = f", σ {stdev:.2f}" if stdev else ""
            plural = "trial" if len(trial_map) == 1 else "trials"

            if show_trial_ids:
                details = ", ".join(f"{tid}:{speed:.2f}" for tid, (speed, _rt) in sorted(trial_map.items()))
                trials_part = f"Trials → {{{details}}}"
            else:
                vals = " ".join(f"{speed:.2f}" for _, (speed, _rt) in sorted(trial_map.items()))
                trials_part = f"Trials → [{vals}]"

            print(
                f"  {MAGENTA}{fuzzer}{RESET}: "
                f"{colour}{mean:.2f}{RESET} exec/s "
                f"({len(trial_map)} {plural}{stdev_part}). "
                f"{trials_part}"
            )

# ───────────────────────────────────────────────────────────────────────────
# chart generation
# ───────────────────────────────────────────────────────────────────────────

def emit_chart(results: Results, out_path: Path, show: bool = False) -> None:
    """Create a grouped bar‑chart and save it to *out_path*."""

    benchmarks = sorted(results.keys())
    fuzzers = sorted({f for v in results.values() for f in v.keys()})

    means = np.full((len(benchmarks), len(fuzzers)), np.nan)
    stds = np.full_like(means, np.nan)

    for bi, bench in enumerate(benchmarks):
        for fi, fuzzer in enumerate(fuzzers):
            if fuzzer in results[bench]:
                _, mean, std = results[bench][fuzzer]
                means[bi, fi] = mean
                stds[bi, fi] = std

    x = np.arange(len(benchmarks))
    width = 0.8 / len(fuzzers)
    offsets = (np.arange(len(fuzzers)) - (len(fuzzers) - 1) / 2) * width

    fig, ax = plt.subplots(figsize=(max(10, len(benchmarks) * 1.8), 6))

    for fi, fuzzer in enumerate(fuzzers):
        bars = ax.bar(
            x + offsets[fi],
            means[:, fi],
            width,
            label=fuzzer.replace("_", " "),
            yerr=stds[:, fi],
            capsize=4,
        )
        ax.bar_label(bars, fmt="%.1f", padding=3, fontsize=8)

    ax.set_ylabel("Executions per second")
    ax.set_title("Throughput (mean ± σ)")
    ax.set_xticks(x)
    ax.set_xticklabels(benchmarks, rotation=30, ha="right")
    ax.legend(title="Fuzzer")
    fig.tight_layout()
    plt.savefig(out_path, dpi=150)
    if show:
        plt.show()
    plt.close(fig)


# ───────────────────────────────────────────────────────────────────────────
# main logic
# ───────────────────────────────────────────────────────────────────────────

def main() -> None:
    global DBG
    args = parse_args()
    DBG = args.debug

    # ------------------------------------------------------------------
    # data source → results dict
    # ------------------------------------------------------------------
    csv_path = None
    if args.csv_path:
        csv_path = Path(args.csv_path).expanduser().resolve()
        if not csv_path.is_file():
            sys.exit(f"CSV '{csv_path}' not found")
        results = read_csv(csv_path)
        out_base = csv_path.parent
    else:
        exp_dir = Path(args.exp_dir).expanduser().resolve()
        exp_name = exp_dir.name
        base_dir = exp_dir / "data" / exp_name / "experiment-folders"
        if not base_dir.is_dir():
            sys.exit(f"Error: '{base_dir}' does not look like a FuzzBench data folder")

        results = _experiment_results(base_dir)
        if not results:
            sys.exit("No valid trials found — nothing to do.")

        # write CSV next to experiment dir for later reuse
        csv_path = exp_dir / "throughput.csv"
        write_csv(results, csv_path)
        out_base = exp_dir

    # ------------------------------------------------------------------
    # presentation (pretty print + chart)
    # ------------------------------------------------------------------
    pretty_print(results, show_trial_ids=args.show_trial_ids)

    png_path = out_base / "throughput.png"
    emit_chart(results, png_path, show=False)
    print(f"\nChart saved to {png_path}")
    if not args.csv_path:
        print(f"CSV saved to {csv_path}")


if __name__ == "__main__":
    main()

